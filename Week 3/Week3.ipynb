{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69cbe65a",
   "metadata": {},
   "source": [
    "   \n",
    "<div style=\"background-image: linear-gradient(to left, rgb(255, 255, 255), rgb(138, 136, 136)); width: auto; margin: 10px;\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/f/fd/University_of_Tehran_logo.svg/225px-University_of_Tehran_logo.svg.png\" width=100px width=auto style=\"padding:10px; vertical-align: center;\">\n",
    "\n",
    "</div>\n",
    "   \n",
    "<div   style:\"text-align: center; background-image: linear-gradient(to left, rgb(255, 255, 255), rgb( 219, 204, 245  ));width: 400px; height: 30px; \">\n",
    "<h1 style=\"font-family: Georgia; color: black; text-align: center; \">SOC AI Course </h1>\n",
    "\n",
    "</div>\n",
    "    <div   style:\"border: 3px solid green;text-align: center; \">\n",
    "<h1 style=\"font-family: Georgia; color: black; text-align: center; \">Week3</h1>\n",
    "<h5 style=\"font-family: Georgia; color: black; text-align: center; \">Designer: Mohammad Amanlou, Marzieh Mousavi</h5>\n",
    "\n",
    "</div>\n",
    "\n",
    "   </html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f27cc",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "In this comprehensive project, you will explore various machine learning algorithms and techniques through practical exercises. The project is designed to provide hands-on experience with key concepts such as Bayes' Theorem, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Logistic Regression, and Recommender Systems. By working through these exercises, you'll gain a deeper understanding of how to preprocess data, develop predictive models, evaluate their performance, and fine-tune them for optimal results.\n",
    "\n",
    "### Structure of the Project\n",
    "\n",
    "The project is divided into four main exercises, each focusing on different aspects of machine learning and data analysis:\n",
    "\n",
    "1. **Exercise 1: Text Classification Using Bayes' Theorem**  \n",
    "   In this exercise, you will apply Bayes' Theorem to classify book descriptions into categories. You'll preprocess the text data, create a Bag of Words model, and use probabilistic reasoning to make predictions. Additionally, you'll explore the impact of stemming and stopword removal on the accuracy of your model.\n",
    "\n",
    "2. **Exercise 2: Traffic Volume Prediction with K-Nearest Neighbors (KNN)**  \n",
    "   This exercise involves predicting traffic volume based on various weather-related features. You'll perform exploratory data analysis, preprocess the dataset, and implement the KNN algorithm. Through model evaluation and fine-tuning, you'll learn how to optimize KNN for better predictive performance.\n",
    "\n",
    "3. **Exercise 3: Recommender Systems with KNN**  \n",
    "   In this exercise, you'll develop a simple recommender system using the KNN algorithm. You'll explore both user-based and item-based collaborative filtering techniques and implement a user-based model using the Surprise library. The exercise will also cover key concepts like cosine similarity and model evaluation.\n",
    "\n",
    "4. **Exercise 4: Titanic Survival Prediction Using SVM and Logistic Regression**  \n",
    "   In the final exercise, you'll work with the famous Titanic dataset to predict passenger survival. You'll develop models using linear and RBF SVM, as well as Logistic Regression. The exercise will guide you through data preprocessing, model development, and evaluation using ROC curves and AUC scores.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By completing this project, you will:\n",
    "- Gain practical experience with essential machine learning algorithms.\n",
    "- Develop skills in data preprocessing, feature engineering, and model evaluation.\n",
    "- Understand the importance of choosing the right model and fine-tuning it for specific tasks.\n",
    "- Learn to apply different visualization techniques to explore and present data insights.\n",
    "- Build a strong foundation in both theoretical concepts and their real-world applications.\n",
    "\n",
    "This project is ideal for those looking to deepen their understanding of machine learning and data science through hands-on practice with real datasets. Whether you're a beginner or an experienced practitioner, these exercises will challenge you to think critically and apply your knowledge to solve complex problems.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 1: Bayes' Theorem\n",
    "\n",
    "### 1. Text Processing and Bayes' Theorem\n",
    "\n",
    "We begin by reviewing the theory of Bayes' Theorem.\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem allows us to calculate the probability of an event based on prior knowledge of conditions related to the event. In other words, it helps us update our belief about the occurrence of an event given new evidence.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$[\n",
    "P(c|x) = \\frac{P(x|c)P(c)}{P(x)}\n",
    "]$\n",
    "\n",
    "Where:\n",
    "- $( P(c|x) )$ is the posterior probability: the probability of class $( c )$ given the data $( x )$.\n",
    "- $( P(x|c) )$ is the likelihood: the probability of the data $( x )$ given that the event belongs to class $( c )$.\n",
    "- $( P(c) )$ is the prior probability: the initial probability of class $( c )$ before seeing the data.\n",
    "- $( P(x) )$ is the evidence: the total probability of the data $( x )$.\n",
    "\n",
    "Using Bayes' Theorem, we can estimate the probability of an event occurring based on prior knowledge and new evidence.\n",
    "\n",
    "### Example of Bayes' Theorem\n",
    "\n",
    "Consider an example with 100 healthy individuals and 100 COVID-19 patients. The test results are as follows:\n",
    "\n",
    "|  | COVID Positive | COVID Negative |\n",
    "|--|--|--|\n",
    "| Healthy | 10 | 90 |\n",
    "| Infected | 95 | 5 |\n",
    "\n",
    "Assume 60% of the population is infected. What is the probability that a person who tested positive is actually healthy?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "1. **Prior Probability**:\n",
    "    - $( P(\\text{Healthy}) = 40\\% )$\n",
    "    - $( P(\\text{Infected}) = 60\\% )$\n",
    "\n",
    "2. **Likelihood**:\n",
    "    - $( P(\\text{Positive} | \\text{Healthy}) = 0.1 )$\n",
    "    - $( P(\\text{Positive} | \\text{Infected}) = 0.95 )$\n",
    "\n",
    "3. **Evidence**:\n",
    "    - Total positive tests: $( 0.4 \\times 0.1 + 0.6 \\times 0.95 )$\n",
    "\n",
    "Using Bayes' formula:\n",
    "\n",
    "$[\n",
    "P(\\text{Healthy} | \\text{Positive}) = \\frac{P(\\text{Positive} | \\text{Healthy}) \\times P(\\text{Healthy})}{P(\\text{Positive})}\n",
    "]$\n",
    "\n",
    "$[\n",
    "P(\\text{Healthy} | \\text{Positive}) = \\frac{0.1 \\times 0.4}{0.4 \\times 0.1 + 0.6 \\times 0.95} = \\frac{0.04}{0.04 + 0.57} = \\frac{0.04}{0.61} = \\frac{4}{61}\n",
    "]$\n",
    "\n",
    "\n",
    "## Project Description\n",
    "\n",
    "In this project, you are given a dataset in CSV format named `books_train.csv` containing book descriptions and their corresponding categories. Another file named `books_test.csv` contains descriptions of books without categories. Your task is to determine the category of each book based on its description.\n",
    "\n",
    "### Phase 0: EDA\n",
    "1. **Part 1**: As part of the initial Exploratory Data Analysis (EDA), create a Word Cloud from the book descriptions to visualize the most frequent words.\n",
    "\n",
    "2. **Part 2**: Analyze unigrams, bigrams, and trigrams before and after removing stopwords. Visualize the frequency of these n-grams using a bar chart to understand the impact of stopword removal.\n",
    "\n",
    "\n",
    "### Phase 1: Data Preprocessing\n",
    "\n",
    "The first phase involves preprocessing the text data. You can use the `hazm` library for this purpose. Manage the data files to ensure the best state for the project. Suggested preprocessing steps include removing punctuation, numbers, and formatting marks, as these do not provide specific information about the category of the books.\n",
    "\n",
    "### Phase 2: Problem Solving\n",
    "\n",
    "Using Bayes' Theorem and the concept of Bag of Words (BoW), solve the problem by determining the category of each book based on its description.\n",
    "\n",
    "#### Bag of Words Concept:\n",
    "\n",
    "Consider two sentences:\n",
    "\n",
    "1. \"I liked the food at this restaurant.\"\n",
    "2. \"The food at this restaurant was very good.\"\n",
    "\n",
    "Using BoW, these sentences can be represented as vectors of word occurrences.\n",
    "\n",
    "Assume the following words:\n",
    "\n",
    "$[ \\text{food, restaurant, liked, good, very, I} ]$\n",
    "\n",
    "Each sentence is then represented as:\n",
    "\n",
    "$[ \\text{Sentence 1: [1, 1, 1, 0, 0, 1]} ]$\n",
    "$[ \\text{Sentence 2: [1, 1, 0, 1, 1, 0]} ]$\n",
    "\n",
    "Using the BoW representation for the descriptions in `books_train.csv`, classify each book in `books_test.csv` using Bayes' Theorem.\n",
    "\n",
    "### Phase 3: Enhanced Analysis\n",
    "\n",
    "1. **Part 1**: Consider the effect of stemming on the accuracy of predictions. Use the `hazm` library to stem words and compare results.\n",
    "\n",
    "2. **Part 2**: Investigate the impact of removing common words (e.g., stopwords) on the accuracy of predictions.\n",
    "\n",
    "3. **Part 3**: Combine the approaches from parts 1 and 2 and analyze the improvement in accuracy.\n",
    "\n",
    "4. **Part 4**: Draw a confusion matrix using sns heatmap for the final model.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Project Description\n",
    "\n",
    "In this project, you will work with the `Metro_Interstate_Traffic_Volume` dataset to predict traffic volume based on various features. The project will involve data visualization, preprocessing, and the application of the K-Nearest Neighbors (KNN) algorithm to develop a predictive model.\n",
    "\n",
    "### Phase 1: Data Exploration\n",
    "\n",
    "Before diving into the predictive modeling, it is essential to explore and visualize the dataset. This phase will help you understand the data distribution, detect outliers, and identify important features.\n",
    "\n",
    "1. **Part 1: Univariate Analysis**\n",
    "   - **Box Plot and Histogram**:\n",
    "     - Select one feature from `temp`, `rain_1h`, or `snow_1h`.\n",
    "     - Draw a Box Plot to visualize the distribution, detect outliers, and understand the feature's spread.\n",
    "     - Draw a Histogram to observe the frequency distribution of the selected feature.\n",
    "   - This analysis will give you insights into the central tendency, variability, and shape of the data distribution for the chosen feature.\n",
    "\n",
    "2. **Part 2: Categorical Feature Analysis**\n",
    "   - **Bar Plot**:\n",
    "     - Select one feature from `clouds_all`, `weather_main`, or `weather_description`.\n",
    "     - Draw a Bar Plot to visualize the frequency of different categories within the selected feature.\n",
    "   - This will help you understand the distribution of categorical variables and their potential impact on traffic volume.\n",
    "\n",
    "### Phase 2: Data Preprocessing\n",
    "\n",
    "Data preprocessing is a critical step to prepare the data for modeling. The following steps are recommended:\n",
    "\n",
    "1. **Handling Missing Data**:\n",
    "   - Identify any missing values and decide on an appropriate strategy to handle them (e.g., imputation, removal).\n",
    "\n",
    "2. **Feature Scaling**:\n",
    "   - Normalize or standardize the numerical features to ensure that all features contribute equally to the distance calculations in KNN.\n",
    "\n",
    "3. **Encoding Categorical Variables**:\n",
    "   - Convert categorical features into numerical values using techniques such as one-hot encoding or label encoding.\n",
    "\n",
    "4. **Splitting the Data**:\n",
    "   - Split the dataset into training and testing sets to evaluate the model's performance on unseen data.\n",
    "\n",
    "5. **Removal of outlier data**: In this section, you can remove outlier data or noise by using one of the methods of removing outlier data. For example, you can use z score. Of course, for this method, you must first choose a reasonable value for z by using appropriate EDA and drawing graphs related to the amount of data removed for each z value. You can also use methods such as removing high or low percentiles according to the data distribution. You can also remove data that is outside the 99% confidence interval. Using any of these methods can be done according to your EDA stage.\n",
    "\n",
    "\n",
    "### Phase 3: Model Development\n",
    "\n",
    "With the data preprocessed, you can now develop a predictive model using the K-Nearest Neighbors (KNN) algorithm.\n",
    "\n",
    "1. **Part 4: KNN Implementation**:\n",
    "   - Implement the KNN algorithm to predict traffic volume based on the selected features. (using libraries)\n",
    "   - Experiment with different values of $( k )$ (number of neighbors) to find the optimal parameter for your model.\n",
    "   \n",
    "2. **Part 5: Model Evaluation**:\n",
    "   - Evaluate the model's performance using appropriate metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
    "   - Compare the results with different values of $( k )$ to determine the best model.\n",
    "\n",
    "### Phase 4: Model Fine-Tuning\n",
    "\n",
    "1. **Part 6: Hyperparameter Tuning**:\n",
    "   - Use techniques such as Grid Search or Random Search to fine-tune the hyperparameters of the KNN model.\n",
    "   - Consider factors like the distance metric (e.g., Euclidean, Manhattan) and the weighting function (uniform or distance-based).\n",
    "\n",
    "2. **Part 7: Model Optimization**:\n",
    "   - Implement cross-validation to further validate the model's performance and ensure it generalizes well to unseen data. (using kfold)\n",
    "\n",
    "### Phase 5: Conclusion\n",
    "\n",
    "1. **Part 8: Final Model Selection**:\n",
    "   - Based on the evaluation metrics and cross-validation results, select the best KNN model for predicting traffic volume.\n",
    "\n",
    "2. **Part 9: Insights and Interpretation**:\n",
    "   - Discuss the insights gained from the analysis and the model's predictions.\n",
    "   - Highlight any potential improvements or future work that could be done to enhance the model's performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 3: Recommender Systems with KNN\n",
    "\n",
    "### Project Description\n",
    "\n",
    "In this exercise, you will develop a simple recommender system using the K-Nearest Neighbors (KNN) algorithm. The project will involve understanding collaborative filtering techniques, implementing a user-based collaborative filtering model, and evaluating its performance.\n",
    "\n",
    "### Phase 1: Understanding Recommender Systems\n",
    "\n",
    "1. **Part 1: Collaborative Filtering Overview**:\n",
    "   - **User-Based Collaborative Filtering**:\n",
    "     - Explain in simple terms how user-based collaborative filtering works. This method recommends items to a user based on the preferences of similar users.\n",
    "   - **Item-Based Collaborative Filtering**:\n",
    "     - Explain how item-based collaborative filtering differs from user-based filtering. This method recommends items similar to those that the user has liked or interacted with before.\n",
    "\n",
    "2. **Part 2: Similarity Metrics**:\n",
    "   - **Cosine Similarity**:\n",
    "     - Explain what cosine similarity is and how it is used to measure the similarity between two vectors (e.g., user preference vectors or item feature vectors).\n",
    "     - Discuss the significance of cosine similarity in the context of collaborative filtering.\n",
    "\n",
    "### Phase 2: Data Preprocessing\n",
    "\n",
    "Before building the recommender system, preprocess the data to ensure it is in the right format.\n",
    "\n",
    "1. **Part 3: Data Preparation**:\n",
    "   - Download the `ratings.csv` dataset, which contains user-item interactions (e.g., ratings).\n",
    "   - Perform Min-Max Scaling on the ratings to normalize the data between 0 and 1.\n",
    "\n",
    "2. **Part 4: Data Splitting**:\n",
    "   - Split the dataset into training and testing sets to evaluate the model's performance on unseen data.\n",
    "\n",
    "### Phase 3: Model Development\n",
    "\n",
    "1. **Part 5: KNN-Based Collaborative Filtering**:\n",
    "   - Implement a user-based collaborative filtering model using the Surprise library and the KNN algorithm.\n",
    "   - Use cosine similarity as the metric to find similar users.\n",
    "\n",
    "2. **Part 6: Model Training**:\n",
    "   - Train the model on the training data and generate predictions for the test set.\n",
    "\n",
    "### Phase 4: Model Evaluation\n",
    "\n",
    "1. **Part 7: Predictions**:\n",
    "   - Print the first five predictions made by the model for the test set.\n",
    "\n",
    "2. **Part 8: Performance Metrics**:\n",
    "   - Evaluate the model's performance using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n",
    "   - Compare the performance with a baseline model (e.g., a model that predicts the average rating).\n",
    "\n",
    "### Phase 5: Conclusion\n",
    "\n",
    "1. **Part 9: Insights and Interpretation**:\n",
    "   - Discuss the strengths and limitations of the KNN-based recommender system.\n",
    "   - Provide insights into how the model could be improved or extended (e.g., by incorporating item-based filtering or hybrid methods).\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 4: SVM, Logistic Regression, and ROC Analysis\n",
    "\n",
    "### Project Description\n",
    "\n",
    "In this project, you will work with the `titanic.csv` dataset to predict which passengers survived the Titanic disaster. The project will involve data exploration, preprocessing, model development using SVM and Logistic Regression, and performance evaluation using ROC curves.\n",
    "\n",
    "### Phase 1: Data Exploration\n",
    "\n",
    "Understanding the data is crucial for building effective models. This phase focuses on exploring the dataset and visualizing important features.\n",
    "\n",
    "1. **Part 1: Feature Analysis**:\n",
    "   - **Boxplot, Histogram, and Barplot**:\n",
    "     - Use the Titanic metadata to understand what each feature represents.\n",
    "     - For each feature, choose one or two visualization techniques (e.g., boxplot, histogram, or barplot) to explore the distribution and relationship with the target variable (survival).\n",
    "   - **Scatter Plot**:\n",
    "     - Draw scatter plots for each feature against the target variable to identify potential correlations.\n",
    "   - **Violin Plot**:\n",
    "     - Create a violin plot for the `age` feature and compare it with a normal distribution to understand its distribution and spread.\n",
    "\n",
    "### Phase 2: Data Preprocessing\n",
    "\n",
    "Prepare the data for modeling by addressing missing values, encoding categorical variables, and scaling features.\n",
    "\n",
    "1. **Part 2: Handling Missing Data**:\n",
    "   - Identify missing values in the dataset and decide on appropriate strategies (e.g., imputation, removal).\n",
    "\n",
    "2. **Part 3: Encoding Categorical Variables**:\n",
    "   - Convert categorical features (e.g., `Sex`, `Embarked`) into numerical values using techniques like one-hot encoding or label encoding.\n",
    "\n",
    "3. **Part 4: Feature Scaling**:\n",
    "   - Normalize or standardize the numerical features to ensure that they contribute equally to the model's decision-making process.\n",
    "\n",
    "4. **Part 5: Data Splitting**:\n",
    "   - Split the dataset into training and testing sets to evaluate the models' performance on unseen data.\n",
    "\n",
    "### Phase 3: Model Development\n",
    "\n",
    "Develop predictive models using SVM (with linear and RBF kernels) and Logistic Regression.\n",
    "\n",
    "1. **Part 6: Linear SVM**:\n",
    "   - Implement a linear SVM model to predict passenger survival.\n",
    "   - Train the model on the training set and evaluate its performance on the test set.\n",
    "\n",
    "2. **Part 7: RBF SVM**:\n",
    "   - Implement an RBF SVM model with a Gaussian kernel.\n",
    "   - Train the model on the training set and evaluate its performance on the test set.\n",
    "\n",
    "3. **Part 8: Logistic Regression**:\n",
    "   - Implement a Logistic Regression model to predict passenger survival.\n",
    "   - Train the model on the training set and evaluate its performance on the test set.\n",
    "\n",
    "### Phase 4: Model Evaluation\n",
    "\n",
    "1. **Part 9: ROC Curve Analysis**:\n",
    "   - Draw the ROC curves for the Linear SVM, RBF SVM, and Logistic Regression models.\n",
    "   - Compare these ROC curves with that of a random guess to evaluate the models' discriminatory power.\n",
    "\n",
    "2. **Part 10: Model Comparison**:\n",
    "   - Identify the best model based on the ROC curves and Area Under the Curve (AUC) values.\n",
    "   - Discuss which model performs the best and why.\n",
    "\n",
    "3. **Part 11: Threshold Analysis**:\n",
    "   - Determine the best decision threshold for the Logistic Regression model to maximize the model's predictive power.\n",
    "\n",
    "### Phase 5: Conclusion\n",
    "\n",
    "1. **Part 12: Final Model Selection**:\n",
    "   - Based on the ROC analysis and threshold optimization, select the best model for predicting Titanic survival.\n",
    "\n",
    "2. **Part 13: Insights and Interpretation**:\n",
    "   - Provide insights into the model's performance, strengths, and limitations.\n",
    "   - Discuss potential areas for improvement or further research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099bcff-44f7-431b-8709-4078601664b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ea16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
